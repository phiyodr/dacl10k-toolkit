{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dacl10k to fiftyone\n",
    "\n",
    "For the install of fiftyone, please refer to: https://docs.voxel51.com/getting_started/install.html.\n",
    "Here's a thumbnail, showing fiftyone after you have run the following two cells:\n",
    "\n",
    "\n",
    "<img src=\"../assets/fiftyone_thumbnail.png\" alt=\"Fiftyone thumbnail\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import fiftyone as fo\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from datetime import date, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 6935/6935 [52.1s elapsed, 0s remaining, 141.0 samples/s]      \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=dc0a1e31-e5f7-4b8d-b725-9e14c65abcd0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f95b3f50df0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create samples for your data\n",
    "images_patt = Path(\"../../../data/dacl10k_dacl-challenge/images/train\") # Path to your images!\n",
    "annotations_patt = Path(\"../../../data/dacl10k_dacl-challenge/annotations/train\") # Path to your annotations!\n",
    "\n",
    "samples = []\n",
    "for annot_path in annotations_patt.iterdir():\n",
    "    if annot_path.is_file() and annot_path.suffix == \".json\":    \n",
    "        with open(annot_path, \"r\") as f:\n",
    "            annot = json.load(f)\n",
    "    \n",
    "    img_path_full_str = str(images_patt / (str(annot_path.stem) + \".jpg\"))\n",
    "    tag = annot['split']\n",
    "\n",
    "    # Metadata:\n",
    "    metadata = fo.ImageMetadata.build_for(img_path_full_str)\n",
    "    sample = fo.Sample(filepath=img_path_full_str, metadata=metadata, tags=[tag],\n",
    "                       created_at=datetime.utcnow(), created_date=date.today(), name=annot_path.stem)\n",
    "\n",
    "    # All polygon dependant info:\n",
    "    detections = []\n",
    "    segmentations = []\n",
    "    classifications_set = set()\n",
    "    for shape in annot[\"shapes\"]:\n",
    "        label = shape[\"label\"]\n",
    "        points = shape[\"points\"]\n",
    "        try:\n",
    "            poly_points = [[(x/annot['imageWidth'],y/annot['imageHeight']) for x,y in points]]\n",
    "        except ValueError as e:\n",
    "            print(\"ValueError: \", annot_path.stem)\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        # Multi-label:\n",
    "        classifications_set.add(label)   \n",
    "        \n",
    "        # Bounding box:\n",
    "        top_left = np.amin(points, 0)\n",
    "        bot_right = np.amax(points,0)\n",
    "        top_left = [top_left[0], \n",
    "                    top_left[1]]\n",
    "        bot_right = [(bot_right[0]),\n",
    "                     (bot_right[1])]\n",
    "        width = bot_right[0]-top_left[0]\n",
    "        height = bot_right[1]-top_left[1]\n",
    "        bbox = [top_left[0]/annot['imageWidth'], top_left[1]/annot['imageHeight'], \n",
    "                width/annot['imageWidth'], height/annot['imageHeight']] # [top-left-x, top-left-y, width, height]\n",
    "        bbox_ = [i.tolist() for i in bbox]\n",
    "        detections.append(\n",
    "            fo.Detection(label=label, bounding_box=bbox, iscrowd=False)\n",
    "        )\n",
    "        segmentations.append(\n",
    "            fo.Polyline(label=label, points=poly_points, closed=True,filled=True, iscrowd=False)\n",
    "        )\n",
    "\n",
    "    # Store detections in a field name of your choice\n",
    "    sample[\"semseg_ground_truth_filled\"] = fo.Polylines(polylines=segmentations)\n",
    "    sample[\"od_ground_truth\"] = fo.Detections(detections=detections)\n",
    "\n",
    "    classifications_list = [fo.Classification(label=label) for label in classifications_set]\n",
    "    sample[\"multi_ground_truth\"] = fo.Classifications(classifications=classifications_list)\n",
    "    \n",
    "    samples.append(sample)\n",
    "\n",
    "# Create dataset\n",
    "dataset = fo.Dataset(\"dacl10k_train\")\n",
    "dataset.add_samples(samples)\n",
    "session = fo.launch_app(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual_semseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
